{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rYGwLxWbQUU9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatma/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1628003628443,
     "user": {
      "displayName": "Fatma Elsafoury",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBOqs1m0p9NKAVLeYOvj2cfPepoOadDs_nVw4YEQ=s64",
      "userId": "12155606853084180649"
     },
     "user_tz": -60
    },
    "id": "UVxSzkPlF_2K"
   },
   "outputs": [],
   "source": [
    "def get_mean_vector(model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    # words: target list words\n",
    "    # returns average vector for all words in target list\n",
    "    import numpy as np\n",
    "    words = [word for word in words if word in model.index_to_key]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model[words], axis=0)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_similarity_Scores(v1,v2):\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    return dot(v1,v2)/(norm(v1)*norm(v2))\n",
    "\n",
    "def normalize_data(a):\n",
    "    normalizded_a = []\n",
    "    amin, amax = min(a), max(a)\n",
    "    for i in a:\n",
    "        normalizded_a.append((i-amin) / (amax-amin))\n",
    "    return normalizded_a\n",
    "    \n",
    "def get_similarity_scores_between_target_words_and_attribute_words(model, attribute_words, target_words):\n",
    "    \n",
    "    similarity_scores_summary = {}\n",
    "    words = []\n",
    "    mean_sim_socres = []\n",
    "\n",
    "    target_words = [word for word in target_words if word in model.index_to_key]\n",
    "    attribute_words = [word for word in attribute_words if word in model.index_to_key]\n",
    "    \n",
    "    if len(target_words) >= 1 and len(attribute_words) >=1:\n",
    "        for i in target_words:\n",
    "            similarity_scores = []\n",
    "            words.append(i)\n",
    "            for j in attribute_words:\n",
    "                cos_sim = get_similarity_Scores(model[i], model[j])\n",
    "                similarity_scores.append(cos_sim)\n",
    "            mean_sim_socres.append(np.mean(normalize_data(similarity_scores)))\n",
    "\n",
    "    similarity_scores_summary[\"word\"] = words\n",
    "    similarity_scores_summary[\"mean_sim_score\"] = mean_sim_socres\n",
    "    return similarity_scores_summary\n",
    "\n",
    "def get_similarity_scores_between_avergare_target_vector_and_attribute_words(model, att_words, target_vector):\n",
    "    # remove out-of-vocabulary words\n",
    "    # att_words: wors in the attribute list\n",
    "    # target_vector: average vector of all words in target group\n",
    "    similarity_scores_summary = {}\n",
    "    words_list = []\n",
    "    similarity_scores = []\n",
    "    words = [word for word in att_words if word in model.index_to_key]\n",
    "    print(len(words))\n",
    "    if len(words) >= 1:\n",
    "        for i in words:\n",
    "            print(i)\n",
    "            words_list.append(i)\n",
    "            cos_sim = get_similarity_Scores(model[i], target_vector)\n",
    "            similarity_scores.append(cos_sim)\n",
    "    similarity_scores_summary[\"words\"] = words_list\n",
    "    similarity_scores_summary[\"cos_sim_scores\"] = similarity_scores\n",
    "    similarity_scores_summary[\"sim_score\"] = normalize_data(similarity_scores)\n",
    "\n",
    "    return similarity_scores_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6yN6B8-RQYh"
   },
   "source": [
    "## Read profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DwpeKwyQP8h3"
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../Data/\"\n",
    "embeddings_folder = data_folder+\"/word embeddings/\"\n",
    "results_folder = \"Results/ethnicity_similarities/word2vec/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profane_words = open(data_folder+\"swear_words.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzAq2cGSRTFv"
   },
   "source": [
    "## read word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x3PE_37vRIxG"
   },
   "outputs": [],
   "source": [
    "def get_google_news_embeddings(filename):\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "    return w2v_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L4uyN9mTKHV"
   },
   "source": [
    "## word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38953,
     "status": "ok",
     "timestamp": 1616509831257,
     "user": {
      "displayName": "Fatma Elsafoury",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBOqs1m0p9NKAVLeYOvj2cfPepoOadDs_nVw4YEQ=s64",
      "userId": "12155606853084180649"
     },
     "user_tz": 0
    },
    "id": "uEofSDjTe9iu",
    "outputId": "aa0f188d-d525-4cb0-9091-1913bf6dc7e6"
   },
   "outputs": [],
   "source": [
    "# convert text to w2v\n",
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#GLOVE_DIR = data_folder+\"/glove.6B.300d_WP.txt\"\n",
    "#w2v_file = data_folder+\"/wv_2_glove.6B.300d_WP.txt\"\n",
    "#glove2word2vec(GLOVE_DIR, w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qrh1lKFdcbuL"
   },
   "outputs": [],
   "source": [
    "w2v_model = get_google_news_embeddings(embeddings_folder+\"/W2V/GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similair words to NOI words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.8543271422386169),\n",
       " ('teenage_girl', 0.7927975654602051),\n",
       " ('woman', 0.7494640946388245),\n",
       " ('teenager', 0.7172499299049377),\n",
       " ('schoolgirl', 0.7075953483581543),\n",
       " ('teenaged_girl', 0.6650916337966919),\n",
       " ('daughter', 0.6489864587783813),\n",
       " ('mother', 0.6478164196014404),\n",
       " ('toddler', 0.6473966836929321),\n",
       " ('girls', 0.6154742240905762)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genderqueer', 0.7026612162590027),\n",
       " ('LGBTQ', 0.6990917325019836),\n",
       " ('gay', 0.6972684860229492),\n",
       " ('LGBT', 0.6857556104660034),\n",
       " ('lesbian', 0.6770454049110413),\n",
       " ('GLBT', 0.6749699711799622),\n",
       " ('Queer', 0.6673474907875061),\n",
       " ('GLBTQ', 0.6604336500167847),\n",
       " ('queers', 0.6523240208625793),\n",
       " ('gay_bisexual_transgendered', 0.6437941193580627)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"queer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homos', 0.6862955093383789),\n",
       " ('homosexuals', 0.6555311679840088),\n",
       " ('queer', 0.6523239612579346),\n",
       " ('lesbians', 0.6243589520454407),\n",
       " ('homophobes', 0.6139503717422485),\n",
       " ('gay', 0.6086617708206177),\n",
       " ('bi_sexuals', 0.608240008354187),\n",
       " ('gays', 0.5921681523323059),\n",
       " ('LGBTs', 0.5905815362930298),\n",
       " ('GLBTs', 0.5848169922828674)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"queers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('immigrants', 0.7985076308250427),\n",
       " ('Immigrant', 0.6984704732894897),\n",
       " ('migrant', 0.6784891486167908),\n",
       " ('illegal_immigrant', 0.6712934970855713),\n",
       " ('immigration', 0.601755678653717),\n",
       " ('Immigrants', 0.6007307171821594),\n",
       " ('Latino_immigrants', 0.5922547578811646),\n",
       " ('undocumented_immigrant', 0.5837289094924927),\n",
       " ('illegal_alien', 0.5803019404411316),\n",
       " ('emigrant', 0.5761232376098633)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"immigrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minority and majority words and profanity vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_majority_words_clean = [\"hetrosexual\", \"cisgender\", \"man\",\"male\",\"boy\",\"son\",\"father\",\"husband\",\"brother\",\"white\",\"caucasian\",\n",
    "                                 \"european american\",\"european\", \"norwegian\", \"canadian\", \"german\", \"australian\", \"english\",\n",
    "                                \"french\", \"american\", \"swedish\", \"dutch\"]\n",
    "\n",
    "target_words = [\"lesbian\",\"gay\",\"bisexual\",\"transgender\",\"tran\",\"queer\",\n",
    "                \"lgbt\",\"lgbtq\",\"homosexual\", \"woman\", \"female\", \"girl\",\"wife\",\"sister\",\"daughter\",\"mother\", \"non-binary\",\n",
    "               \"african\", \"african american\", \"asian\", \"black\", \"hispanic\", \"latin\", \"mexican\", \"indian\", \"middle eastern\",\n",
    "                \"arab\"]\n",
    "\n",
    "words = harmless_majority_words_clean + target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "hetrosexual\n",
      "man\n",
      "male\n",
      "boy\n",
      "son\n",
      "father\n",
      "husband\n",
      "brother\n",
      "white\n",
      "caucasian\n",
      "european\n",
      "norwegian\n",
      "canadian\n",
      "german\n",
      "australian\n",
      "english\n",
      "french\n",
      "american\n",
      "swedish\n",
      "dutch\n",
      "lesbian\n",
      "gay\n",
      "bisexual\n",
      "transgender\n",
      "tran\n",
      "queer\n",
      "lgbt\n",
      "homosexual\n",
      "woman\n",
      "female\n",
      "girl\n",
      "wife\n",
      "sister\n",
      "daughter\n",
      "mother\n",
      "african\n",
      "asian\n",
      "black\n",
      "hispanic\n",
      "latin\n",
      "mexican\n",
      "indian\n",
      "arab\n"
     ]
    }
   ],
   "source": [
    "profane_vector = get_mean_vector(w2v_model, profane_words)\n",
    "vec_majority_sim_dict = get_similarity_scores_between_avergare_target_vector_and_attribute_words(w2v_model, words, profane_vector)\n",
    "pd.DataFrame(vec_majority_sim_dict).to_csv(results_folder+\"_minority_and_majority_words_sim_to_profane_vector.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(25, 10), dpi=1200)\n",
    "sorted_index = np.argsort(vec_majority_sim_dict[\"sim_score\"])\n",
    "\n",
    "x_axis = np.arange(len(vec_majority_sim_dict[\"words\"]))\n",
    "y_axis = [vec_majority_sim_dict[\"sim_score\"][i] for i in sorted_index]\n",
    "x_ticks = [vec_majority_sim_dict[\"words\"][i] for i in sorted_index]\n",
    "plt.bar(x_axis, y_axis)\n",
    "plt.xticks(x_axis, x_ticks, fontsize=40, fontweight='bold', rotation=90)\n",
    "plt.ylabel(\"similarity score\", fontsize=40, fontweight='bold')\n",
    "plt.yticks(np.arange(0.1,1.1,0.1),fontsize=40, fontweight='bold')\n",
    "plt.savefig(results_folder+\"similarity_score_of_minority_and_majority_words_to_profanity_vector.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPF4uvqe/FlFmRUzJlu6T4C",
   "collapsed_sections": [],
   "name": "semantic_word_similarity_different_WE_w2v.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
